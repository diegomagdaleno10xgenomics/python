TF GPT2

Description: Train GPT-2 model with Tensorflow and Huggingface transformers. In this case, we are training GPT-2 to generate abstracts similar to those found on papers in arxiv.org. This uses the arxiv metadata dataset from Kaggle.

Extra Resources/Examples: 
 - https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272
 - https://towardsdatascience.com/natural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a
 - https://colab.research.google.com/github/tfindiamooc/tfindiamooc.github.io/blob/master/colabs/text.ipynb#scrollTo=YWVWjyIkffau
